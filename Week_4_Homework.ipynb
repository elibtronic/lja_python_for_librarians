{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Student Name\n",
    " \n",
    "Please add your name to this markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting your homework!\n",
    "\n",
    "Three steps are required for this week's homework once you have completed it:\n",
    "\n",
    "- __Share__ this notebook with _libraryjuicepresspython@gmail.com_ so it can be marked in Colab\n",
    "- __Print as PDF__ and upload to the the week's homework submission location on the Library Juice Academy LMS\n",
    "- __Post__ to the Week 4 Forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Librarians - Week 4 Homework\n",
    "\n",
    "## Machine Learning Madness\n",
    "\n",
    "We are going to take our new knowledge of ML and apply it to a different dataset. This week we are going to look at a concatenated version of this [dataset](https://datadryad.org/stash/dataset/doi:10.5061/dryad.2h4j5). \n",
    "\n",
    "Let's see if you can infer what our model features and target will be once we look at the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#We'll draw a graph later on\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Our 'Machine Learning pieces'\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_text\n",
    "from sklearn import metrics \n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "#Suppress the distracting warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load up our data\n",
    "citation_data = pd.read_csv(\"https://raw.githubusercontent.com/elibtronic/lja_datasets/master/week_4_homework_citation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell a few times to get a look at the data\n",
    "citation_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to generate some summary statistics on the data\n",
    "citation_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is ~1300 lines from the original dataset with the following columns:\n",
    "\n",
    "- Score1 - a score assigned by assesor 1\n",
    "- Score2 - a score assigned by assessor 2\n",
    "- IF2 - the two year impact factor score \n",
    "- IF5 - the five year impact factor score\n",
    "- TopCitation - If the citation is among the top 10% of all citations in this dataset\n",
    "\n",
    "We are going to build a **Decision Tree Classifer** ML model. We'll use that model to see if we can predict if a citation from the dataset will be in the top 10% of all cited articles.\n",
    "\n",
    "This seems like an esoteric question to ask and answer? Yes, I'll give you that. We are trying to see if 4 different _scores_ can be used to determine what class we can put a citation in: \n",
    "\n",
    "- The top 10% (those worth a closer look)\n",
    "- The bottom 90% (those that we don't have to look at any closer)\n",
    "\n",
    "We are also going to see how accurate we can get a model by tweaking some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncommenting\n",
    "\n",
    "Up until this point we have been using comments to leave notes to the people that read the code. We can also use comments to block out different lines of code we don't want Python to run. The following cell has an example. Remove the correct comment symbol `#` so that the code executes. We'll use uncommenting in _Q1_ to select our features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_a_year = 365\n",
    "#print(days_in_a_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "In the next cell uncomment the correct lines to identify the features and the targets that this model will be built with. \n",
    "\n",
    "You need to uncomment 1 line between lines: 3 & 10 and uncomment one line between lines: 14 & 18. Your answer for _Q1_ will be the uncommented lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which set of columns will be our features?\n",
    "\n",
    "#citation_features = [\"Score1\"]\n",
    "#citation_features = [\"Score1\",\"Score2\",\"IF2\",\"IF5\",\"TopCitation\"]\n",
    "#citation_features = [\"Score1\",\"Score2\"]\n",
    "#citation_features = [\"Score1\",\"Score2\",\"IF2\",\"IF5\"]\n",
    "#citation_features = [\"Score1\",\"Score2\",\"IF2\"]\n",
    "#citation_features = [\"TopCitation\"]\n",
    "#citation_features = [\"TopCitation\",\"Score1\"]\n",
    "#citation_features = [\"TopCitation\",\"IF2\",\"IF5\"]\n",
    "\n",
    "#Which column will be our target?\n",
    "\n",
    "#citation_target = [\"Score1\"]\n",
    "#citation_target = [\"Score2\"]\n",
    "#citation_target = [\"IF2\"]\n",
    "#citation_target = [\"IF5\"]\n",
    "#citation_target = [\"TopCitation\"]\n",
    "\n",
    "X = citation_data[citation_features]\n",
    "y = citation_data[citation_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to build the Decision Tree Classifier model and get the accuracy of the model using our basic set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll start with 20 just for fun\n",
    "test_percent = 20\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \\\n",
    "                                                    y, \\\n",
    "                                                    test_size=test_percent/100.0)\n",
    "# Create Decision Tree classifer object\n",
    "treeClass = DecisionTreeClassifier()\n",
    "\n",
    "# Train\n",
    "treeClass = treeClass.fit(X_train,y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred = treeClass.predict(X_test)\n",
    "\n",
    "#Accuracy?\n",
    "print(\"Accuracy of our model: \")\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing our tree\n",
    "\n",
    "Let's have a look at the tree we created without changing any hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printed_tree = export_text(treeClass,feature_names=citation_features)\n",
    "print(printed_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Let's see what effect changing the testing percent has on accuracy. In the cell below add some values in the `test_percents` list to test this and view the corresponding graph. You need to modify line 3.\n",
    "\n",
    "Your graph will be the answer to _Q2_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add some values between 1 - 99 in a comman separated list in the next line\n",
    "\n",
    "testing_percents = [,,,,]\n",
    "\n",
    "accuracy = []\n",
    "training_percents = []\n",
    "\n",
    "for test_ratio in sorted(testing_percents):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \\\n",
    "                                                        y, \\\n",
    "                                                        test_size=test_ratio/100.0)\n",
    "    treeClassTest = DecisionTreeClassifier()\n",
    "    treeClassTest = treeClassTest.fit(X_train,y_train)\n",
    "    y_pred = treeClassTest.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test,y_pred)\n",
    "    accuracy.append(score)\n",
    "    training_percents.append(100 - test_ratio)\n",
    "\n",
    "    \n",
    "plt.plot(training_percents,accuracy)\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.xlabel(\"Training Size %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "Let's wee what effect changing the maximum depth has on accuracy. In the cell below add some values in the max_options list to test this and view the corresponding graph. You need to modify line 6.\n",
    "\n",
    "Your graph will be the answer to _Q3_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll fix this at 20% for this investigation\n",
    "test_percent = 20\n",
    "\n",
    "#add some values between 1 - 30 in a comma separated list in the next line\n",
    "\n",
    "max_options = [,,,,]\n",
    "\n",
    "accuracy = []\n",
    "tree_max = []\n",
    "\n",
    "for max_d in sorted(max_options):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \\\n",
    "                                                        y, \\\n",
    "                                                        test_size=test_percent/100.0)\n",
    "    \n",
    "    #We set maximum depth in the DecisionTreeClassifer when we first create the variable\n",
    "    treeClassTest = DecisionTreeClassifier(max_depth=max_d)\n",
    "    treeClassTest = treeClassTest.fit(X_train,y_train)\n",
    "    y_pred = treeClassTest.predict(X_test)\n",
    "    score = metrics.accuracy_score(y_test,y_pred)\n",
    "    accuracy.append(score)\n",
    "    tree_max.append(max_d)\n",
    "\n",
    "    \n",
    "plt.plot(max_options,accuracy)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Maximum Depth of Tree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing and minimizing our accuracy\n",
    "\n",
    "Use the cell below to answer _Q5_. You can modify the values on line 2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_percent = \n",
    "max_d = \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \\\n",
    "                                                    y, \\\n",
    "                                                    test_size=test_percent/100.0)\n",
    "    \n",
    "treeClassTest = DecisionTreeClassifier(max_depth=max_d)\n",
    "treeClassTest = treeClassTest.fit(X_train,y_train)\n",
    "y_pred = treeClassTest.predict(X_test)\n",
    "\n",
    "#Accuracy?\n",
    "print(\"Calculated accuracy: \")\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "\n",
    "print(\"\\nTree generated\")\n",
    "printed_tree = export_text(treeClass,feature_names=citation_features)\n",
    "print(printed_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "What combination of parameters above led to the highest accuracy? Don't worry if you can't get a perfect tree. The goal of this question is to experiment with the 2 parameters to see if you can modify both at once to get a better score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the highest accuracy by setting\n",
    "\n",
    "- Testing percentage to:\n",
    "- Maximimum depth of the tree to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Trees to Forests\n",
    "\n",
    "We've explored the accuracy of our ML model when we just created one tree. Let's see if we can increase this with a forest of trees for our citation information. Try some different values in for the two parameters on line 1 and line 2 to answer _Q6_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pick a values between 1 and 99\n",
    "test_percent = \n",
    "#Pick a values between 10 - 50\n",
    "number_estimators = \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \\\n",
    "                                                    y, \\\n",
    "                                                    test_size=test_percent/100.0)\n",
    "\n",
    "#Create Random Classifier\n",
    "clf = RandomForestClassifier(n_estimators=number_estimators)\n",
    "\n",
    "#Train\n",
    "clf.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "#Predict\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "#Accuracy\n",
    "print(\"Accuracy?\")\n",
    "print(metrics.accuracy_score(y_train,y_pred))\n",
    "\n",
    "\n",
    "#Visualize the first tree in this forest\n",
    "print(\"\\nFirst tree in this forest\")\n",
    "printed_tree = export_text(clf.estimators_[0],feature_names = citation_features)\n",
    "print(printed_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "What combination of parameters above led to the highest accuracy? Don't worry if you can't get a perfect forest/tree. The goal of this question is to experiment with the 2 parameters to see if you can modify both at once to get a better score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the highest accuracy by setting\n",
    "\n",
    "- Testing percentage to:\n",
    "- Number of estimators to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "\n",
    "Which model (**decision tree** or **forest**) was easier to maximum accuracy for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that was easiest to maximize was..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8\n",
    "\n",
    "Can you take a guess as to why your answer to _Q7_ was the way it was?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it was because..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You've now officially completed Python for Librarians! Be sure to complete the last three steps:\n",
    "\n",
    "- Save your notebook as PDF and upload\n",
    "- Share your completed notebook with `libraryjuicepresspython@gmail.com` so I can get a shared copy of your work.\n",
    "- Head over to the Week 4 Homework Forum to make your last post\n",
    "\n",
    "-----\n",
    "\n",
    "# Thanks! I hope you enjoyed this\n",
    "\n",
    "Thanks for taking this class and giving Python a try. I have put together all of the datasets I've used for this class in a [GitHub repository](https://github.com/elibtronic/lja_datasets) Feel free to open up a fresh Google Collab notebook and load a CSV file to perform some analysis or to build some machine learning models. \n",
    "\n",
    "Please drop me a [line](https://twitter.com/elibtronic/) if you'd like to talk more about these topics or if you have a notebook that you'd like to share."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
